# INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection (ICLR-2024)

<div align=center><img src="https://github.com/alibaba/eigenscore/blob/main/data/datasets/fig.png" width="750" /></div>


* This repository contains code for our paper **INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection** [Download paper here]([https://arxiv.org/abs/1912.11976](https://arxiv.org/pdf/2402.03744))
* If you have any question about our paper or code, please don't hesitate to contact with me chench@zju.edu.cn/ercong.cc@alibaba-inc.com, we will update our repository accordingly

## Setup
### **Dataset** 
The dataset can be downloaded here ...

### **Requirements**

## Run the code

## Use Eigensocre in your code

## Citation
```
@article{chen2024inside,
title={INSIDE: LLMs' Internal States Retain the Power of Hallucination Detection},
author={Chen, Chao and Liu, Kai and Chen, Ze and Gu, Yi and Wu, Yue and Tao, Mingyuan and Fu, Zhihang and Ye, Jieping},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
```

